{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.optimize import AdamOptimizer,QNSPSAOptimizer,AdagradOptimizer\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error as mse\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import clear_output\n",
    "import random \n",
    "from pennylane.optimize import AdamOptimizer,QNSPSAOptimizer\n",
    "from utils import *\n",
    "from autoencoder import Autoencoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os \n",
    "\n",
    "custom_palette =['#EABFCB','#C191A1','#A4508B','#5F0A87','#2F004F','#120021',]\n",
    "range_qubit_autoencoder=list(range(2,8))\n",
    "range_batches = [10,20,50,100]\n",
    "seed=42\n",
    "epochs=150\n",
    "n=100\n",
    "stepsize=.2\n",
    "opt=AdamOptimizer(stepsize=.2)\n",
    "X=np.random.rand(n)*2*np.pi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_following_elements(lst, eta):\n",
    "    result = [lst[0]]  \n",
    "    i = 0  \n",
    "    while i < len(lst) - 1:\n",
    "        if abs(lst[i] - lst[i + 1]) > eta:\n",
    "            result.append(lst[i + 1])  # Add the next element if the condition holds\n",
    "        i += 1  # Move to the next element\n",
    "    \n",
    "    return [0 if abs(x) <= 0.000001 else x for x in result]\n",
    "\n",
    "def get_eigen_loss_values(X,qb_input_state,qb_trash_state):\n",
    "    a=[]\n",
    "    for theta in X:\n",
    "        a.append(look_Sss(theta,qb_input_state))\n",
    "    b =np.sum([tensor(c, c.dag()) for c in a])/len(a)\n",
    "    \n",
    "    c=[]\n",
    "    for j in range(pow(2,qb_input_state-qb_trash_state)):\n",
    "        c.append(1-np.sum(b.eigenenergies()[-j-1:]))\n",
    "\n",
    "\n",
    "    return [1-np.sum(b.eigenenergies()[-a-1:]) for a in range(pow(2,qb_input_state-qb_trash_state))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m min_found\u001b[38;5;241m=\u001b[39m {a:\u001b[38;5;28mmin\u001b[39m(batch_losses[a]) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m range_batches }\n\u001b[0;32m     11\u001b[0m min_loss \u001b[38;5;241m=\u001b[39mget_min_loss_fid(X,n_qubit_autoencoder,n_trash_qubit)\n\u001b[1;32m---> 12\u001b[0m eigen_loss \u001b[38;5;241m=\u001b[39m\u001b[43mget_eigen_loss_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_qubit_autoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_trash_qubit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# plt.figure()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# sns.set_palette(custom_palette)  \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# for b,r in zip(list(batch_losses.values()),range_batches):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# plt.xlabel('epochs')\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# plt.ylabel('loss')\u001b[39;00m\n\u001b[0;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss on AE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_qubit_autoencoder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_trash_qubit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[48], line 13\u001b[0m, in \u001b[0;36mget_eigen_loss_values\u001b[1;34m(X, qb_input_state, qb_trash_state)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_eigen_loss_values\u001b[39m(X,qb_input_state,qb_trash_state):\n\u001b[0;32m     12\u001b[0m     a\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m theta \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSX\u001b[49m:\n\u001b[0;32m     14\u001b[0m         a\u001b[38;5;241m.\u001b[39mappend(look_Sss(theta,qb_input_state))\n\u001b[0;32m     15\u001b[0m     b \u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum([tensor(c, c\u001b[38;5;241m.\u001b[39mdag()) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m a])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(a)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SX' is not defined"
     ]
    }
   ],
   "source": [
    "for n_qubit_autoencoder in range_qubit_autoencoder:\n",
    "    for n_trash_qubit in range(1,n_qubit_autoencoder):\n",
    "        img_folder=f'runs/run_{n_qubit_autoencoder}to{n_trash_qubit}'\n",
    "        batch_losses={}\n",
    "        for batch_size in range_batches:\n",
    "            folder=img_folder+f'/{batch_size}'\n",
    "            loss=np.load(folder+'/loss.npy')            \n",
    "            batch_losses[batch_size]=loss\n",
    "\n",
    "        min_found= {a:min(batch_losses[a]) for a in range_batches }\n",
    "        min_loss =get_min_loss_fid(X,n_qubit_autoencoder,n_trash_qubit)\n",
    "        eigen_loss =get_eigen_loss_values(X,n_qubit_autoencoder,n_trash_qubit)\n",
    "        # plt.figure()\n",
    "        # sns.set_palette(custom_palette)  \n",
    "        # for b,r in zip(list(batch_losses.values()),range_batches):\n",
    "        #     sns.lineplot(x=range(epochs),y=b,label=r)\n",
    "        # plt.legend(title='Batch size')\n",
    "        # for i, a in enumerate(eigen_loss):\n",
    "        #     plt.hlines(a,0,epochs-1,color='#AE687A',linestyle='--')\n",
    "        #     plt.text(x=epochs//(len(eigen_loss))*i+epochs//(len(eigen_loss)), y=a+0.05, fontsize='x-small', s=f'loss@eigen {i}', color='#773344', ha='right', va='center')\n",
    "\n",
    "        # plt.hlines(min_loss,0,epochs-1,color='#773344',linestyle='--')\n",
    "        # plt.text(x=epochs//2, y=min_loss+0.05, fontsize='medium', s=f'Min loss', color='#773344', ha='right', va='center')\n",
    "        # plt.ylim((0,1))\n",
    "\n",
    "\n",
    "        \n",
    "        # plt.xlabel('epochs')\n",
    "        # plt.ylabel('loss')\n",
    "        plt.title(f'Loss on AE {n_qubit_autoencoder}->{n_trash_qubit}')\n",
    "        # plt.savefig(img_folder+f'/{n_qubit_autoencoder}_{n_trash_qubit}')\n",
    "        # Info file \n",
    "        # with open(img_folder+f'/info.txt','a') as file:\n",
    "        #     file.write(f'RUN INFORMATION\\nInput qubits={n_qubit_autoencoder}\\nTrash qubit={n_trash_qubit}\\nSeed={seed}\\nOptimizer=AdamOptimizer(stepsize={opt.stepsize})\\nEpochs={epochs}\\nBatch sizes={range_batches}\\nMin fidelity loss={min_loss}\\nMin loss found=\\n{''.join([f'\\t\\t\\t\\t{a}\\t:\\t{b}\\n' for a, b in min_found.items()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
