{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.optimize import AdamOptimizer,QNSPSAOptimizer,AdagradOptimizer\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error as mse\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import clear_output\n",
    "import random \n",
    "from pennylane.optimize import AdamOptimizer,QNSPSAOptimizer\n",
    "from utils import *\n",
    "from autoencoder import Autoencoder\n",
    "from autoencoder2 import Autoencoder_c11\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os \n",
    "from autoencoder3 import Autoencoder_composite\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette =[\n",
    "    # '#C24AA2','#D6518F',\n",
    "    '#EC5A77','#F57C73','#F69C6D','#F6BC66']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_qubit_autoencoder=list(range(2,8))\n",
    "range_batches = [10,20,50,100]\n",
    "seed=42\n",
    "epochs=[100,50]\n",
    "n=150\n",
    "stepsize=.2\n",
    "opt=AdamOptimizer(stepsize=.2)\n",
    "X=np.random.rand(n)*2*np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AE with 2 input qubit and 1 trash qubit in batches of 10\n",
      "Stage: 0, \tEpoch 18, \tBatch:9, \tTrain Loss = 0.000485, \tVal Loss = 0.000518\n",
      "Early stop\n",
      "Stage: 1, \tEpoch 7, \tBatch:9, \tTrain Loss = 0.000533, \tVal Loss = 0.000522\n",
      "Early stop\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Running AE with 2 input qubit and 1 trash qubit in batches of 20\n",
      "Stage: 0, \tEpoch 16, \tBatch:4, \tTrain Loss = 0.000606, \tVal Loss = 0.000578\n",
      "Early stop\n",
      "Stage: 1, \tEpoch 7, \tBatch:4, \tTrain Loss = 0.001295, \tVal Loss = 0.001331\n",
      "Early stop\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Running AE with 2 input qubit and 1 trash qubit in batches of 50\n",
      "Stage: 0, \tEpoch 22, \tBatch:1, \tTrain Loss = 0.001007, \tVal Loss = 0.000886\n",
      "Early stop\n",
      "Stage: 1, \tEpoch 30, \tBatch:1, \tTrain Loss = 0.000987, \tVal Loss = 0.000949\n",
      "Early stop\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Running AE with 2 input qubit and 1 trash qubit in batches of 100\n",
      "Stage: 0, \tEpoch 64, \tBatch:0, \tTrain Loss = 0.000653, \tVal Loss = 0.000602\n",
      "Early stop\n",
      "Stage: 1, \tEpoch 19, \tBatch:0, \tTrain Loss = 0.000375, \tVal Loss = 0.000348\n",
      "Early stop\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Running AE with 3 input qubit and 1 trash qubit in batches of 10\n",
      "Stage: 0, \tEpoch 44, \tBatch:9, \tTrain Loss = 0.000725, \tVal Loss = 0.000746\n",
      "Early stop\n",
      "Stage: 1, \tEpoch 7, \tBatch:9, \tTrain Loss = 0.000677, \tVal Loss = 0.000653\n",
      "Early stop\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Running AE with 3 input qubit and 1 trash qubit in batches of 20\n",
      "Stage: 0, \tEpoch 5, \tBatch:3, \tTrain Loss = 0.178748, \tVal Loss = 0.178130"
     ]
    }
   ],
   "source": [
    "for n_qubit_autoencoder in range_qubit_autoencoder:\n",
    "    for n_trash_qubit in range(1,n_qubit_autoencoder):\n",
    "        train_batch_losses={}\n",
    "        val_batch_losses={}\n",
    "        batch_times={}\n",
    "        img_folder=f'runs/run_{n_qubit_autoencoder}to{n_qubit_autoencoder-n_trash_qubit}'\n",
    "        os.makedirs(img_folder,exist_ok=True)\n",
    "        for batch_size in range_batches:\n",
    "            folder=img_folder+f'/{batch_size}'\n",
    "            print(f\"Running AE with {n_qubit_autoencoder} input qubit and {n_trash_qubit} trash qubit in batches of {batch_size}\")\n",
    "            n_qubit=n_qubit_autoencoder\n",
    "            dvc = qml.device('default.qubit', wires=n_qubit, shots=None)\n",
    "            ae = Autoencoder_composite(n_qubit_autoencoder,n_trash_qubit,dvc,stages=['c6','c11'])\n",
    "\n",
    "            start_time = time.time()\n",
    "            ae.train(X,opt,epochs,batch_size,val_split=.33)\n",
    "            end_time = time.time()\n",
    "\n",
    "            os.makedirs(folder)\n",
    "            train_loss,val_loss=ae.get_loss()\n",
    "            train_batch_losses[batch_size]=train_loss\n",
    "            val_batch_losses[batch_size]=val_loss\n",
    "            batch_times[batch_size]=end_time-start_time\n",
    "            weights=ae.best_params()\n",
    "            np.save(folder+'/loss_train',np.array(train_loss))            \n",
    "            np.save(folder+'/loss_val',np.array(val_loss))            \n",
    "            np.save(folder+'/weights',np.array(weights))\n",
    "\n",
    "        # Min loss\n",
    "        min_val_found= {a:min(val_batch_losses[a]) for a in range_batches }\n",
    "        min_train_found= {a:min(train_batch_losses[a]) for a in range_batches }\n",
    "        min_loss =get_min_loss_fid(X,n_qubit_autoencoder,n_trash_qubit)\n",
    "\n",
    "        # Figure\n",
    "        plt.figure()\n",
    "        sns.set_palette(custom_palette)  \n",
    "        for a,b,c,d in zip(list(train_batch_losses.values()),list(val_batch_losses.values()),range_batches,custom_palette):\n",
    "            sns.lineplot(x=range(len(a)),y=a,label=f'train_{c}',color=d)\n",
    "            sns.lineplot(x=range(len(b)),y=b,label=f'val_{c}', color=d,linestyle=':')\n",
    "        plt.legend(title='Batch size')\n",
    "        plt.vlines(epochs[:-1],0,1,color='#C24AA2',linestyle='--')\n",
    "        plt.hlines(min_loss,0,sum(epochs)-1,color='#773344',linestyle='--')\n",
    "        plt.text(x=sum(epochs)//3*2, y=min_loss+0.05, fontsize='medium', s=f'Min loss', color='#773344', ha='right', va='center')\n",
    "        plt.text(epochs[-1]-0.02, y=0.85, fontsize='medium', s=f'stage\\nchange', color='#973C7F', ha='center', va='center')\n",
    "        plt.ylim((0,1))\n",
    "\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title(f'Loss on AE {n_qubit_autoencoder}->{n_qubit_autoencoder-n_trash_qubit}')\n",
    "        exit()\n",
    "        plt.savefig(img_folder+f'/{n_qubit_autoencoder}_{n_qubit_autoencoder-n_trash_qubit}')\n",
    "        # Info file \n",
    "        with open(img_folder+f'/info.txt','a') as file:\n",
    "            file.write(f'RUN INFORMATION\\nInput qubits={n_qubit_autoencoder}\\nTrash qubit={n_trash_qubit}\\nSeed={seed}\\nOptimizer=AdamOptimizer(stepsize={opt.stepsize})\\nEpochs={epochs}\\nBatch sizes={range_batches}\\nMin fidelity loss={min_loss}\\nMin val loss found=\\n{''.join([f'\\t\\t\\t\\t{a}\\t:\\t{b}\\n' for a, b in min_val_found.items()])}\\nExec (training) time=\\n{''.join([f'\\t\\t\\t\\t{a}\\t:\\t{b}\\n' for a, b in batch_times.items()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_update_accumulation',\n",
       " 'accumulation',\n",
       " 'apply_grad',\n",
       " 'beta1',\n",
       " 'beta2',\n",
       " 'compute_grad',\n",
       " 'eps',\n",
       " 'fm',\n",
       " 'reset',\n",
       " 'sm',\n",
       " 'step',\n",
       " 'step_and_cost',\n",
       " 'stepsize',\n",
       " 't']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
